---
title: 'Project 6: Randomization and Matching'
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r}
# Load tidyverse and MatchIt
# Feel free to load other libraries as you wish
library(tidyverse)
library(MatchIt)


# Load ypsps data
ypsps <- read.csv('data/ypsps.csv')
head(ypsps)
```

# Randomization

Matching is usually used in observational studies to to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}

```{r}
# Generate a vector that randomly assigns each unit to treatment/control
set.seed(1234)
ypsps <- ypsps %>%
  mutate(treatment = sample(c(0, 1), nrow(ypsps), replace = TRUE))

# Choose a baseline covariate (use dplyr for this)
baseline_covariate <- ypsps %>%
  select(parent_OwnHome)

  #install.packages("cli", type = "source")

# Visualize the distribution by treatment/control (ggplot)
baseline_covariate %>%
  mutate(treatment = ypsps$treatment) %>%
  ggplot(aes(x = parent_OwnHome, fill = factor(treatment))) +
  geom_bar(position = "dodge") +
  labs(title = "Distribution of Parent Home Ownership by Treatment/Control",
       x = "Parent Home Ownership",
       y = "Count",
       fill = "Treatment") +
  theme_minimal()


# Simulate this 10,000 times (monte carlo simulation - see R Refresher for a hint)

nsim <- 10000

sim_results <- replicate(nsim, {
  # Randomly assign treatment
  ypsps <- ypsps %>%
    mutate(treatment = sample(c(0, 1), nrow(ypsps), replace = TRUE))
  
    # Calculate means for treatment and control
  means <- ypsps %>%
    group_by(treatment) %>%
    summarize(mean_parent_OwnHome = mean(parent_OwnHome, na.rm = TRUE)) %>%
    pull(mean_parent_OwnHome)
  
  #Return differences
  abs_diff <- abs(diff(means))
})

sim_df <- data.frame(balance_diff = sim_results)
# Visualize the distribution of balance across simulations
# Plot histogram of balance differences
ggplot(sim_df, aes(x = balance_diff)) +
  geom_histogram(bins = 50, fill = "skyblue", color = "white") +
  labs(
    title = "Distribution of Absolute Differences in Parent Home Ownership",
    x = "Absolute Difference in Means (Treatment vs Control)",
    y = "Frequency"
  ) +
  theme_minimal() +
  geom_vline(aes(xintercept = mean(balance_diff)), color = "red", linetype = "dashed")
```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

Your Answer: The simulations show that in general, covariates are well balanced, always below the threshold we use later for determining balance. Still, it shows the variation that occurs in random sampling, where we see that some random draws are demonstrate perfect balance, and others are less well balanced. Sampling variability which is inherent in random sampling means that for any given sample, the treatment and control groups may not be perfectly balanced on all covariates, despite expecting them to be balanced in the population. Random assignment only ensures that, on average, treatment and control groups will be balanced across many samples, but not necessarily in any single sample. These variations over samples are random, i.e. the difference in the covariate between treatment and control groups is due to chance.

# Propensity Score Matching

## One Model
Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Plot the balance of the top 10 (or fewer if you select fewer covariates). Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.

```{r}
# Select covariates that represent the "true" model for selection, fit model

# Estimate the propensity score model
ps_model <- matchit(
  college ~ parent_OwnHome + parent_Employ + parent_EducHH + 
    student_GPA + student_SchOfficer + student_FPlans,
  data = ypsps,
  method = "nearest",      # use nearest neighbor matching
  distance = "logit"       # default, logistic regression
)

# Summary includes standardized mean differences
summary_stats <- summary(ps_model, standardize = TRUE)

#install.packages("cobalt")
library(cobalt)

# Plot the balance for the top 10 covariates
# Plot top 10 covariates or fewer
love.plot(ps_model, 
          threshold = 0.1,
          abs = TRUE,
          var.order = "unadjusted",
          stat = "mean.diffs",
          title = "Covariate Balance Before and After Matching")

# Report the overall balance and the proportion of covariates that meet the balance threshold of $\leq .1$

summary(ps_model, standardize = TRUE)$sum.matched

```
Answer: None of the chosen covariates meet the desired balance threshold



## Simulations

Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r}
#packages
library(purrr)
library(ggplot2)
library(gridExtra)

nsim_models <- 10000

# Remove post-treatment covariates

clean_df <- ypsps %>%
  select(-starts_with("student_1973"), -starts_with("student_1982")) 

#drop "placebo" vars (creating errors)

clean_df <- clean_df %>%
  select(-ends_with("placebo"))

#drop duplicate outcome vars

clean_df <- clean_df %>%
  select(-student_vote, -student_meeting, -student_button, -student_money, -student_communicate, -student_demonstrate, -student_community, -student_other, -treatment)

# Randomly select features
set.seed(42)
treatment_var <- "college"
outcome_var <- "student_ppnscal"
covariates <- setdiff(colnames(clean_df), c(treatment_var, outcome_var))
run_sim <- function() {
# Simulate random selection of features 10k+ times
  # Randomly select a number of covariates
  num_covariates <- sample(1:length(covariates), 1)
  
  # Randomly select covariates
  selected_covariates <- sample(covariates, num_covariates)
  
  # Create formula for matching
  formula <- as.formula(paste(treatment_var, "~", paste(selected_covariates, collapse = "+")))

# Fit p-score models and save ATTs, proportion of balanced covariates, and mean percent balance improvement  
  # Fit the propensity score model
  ps_model <- suppressWarnings(
    matchit(formula, data = clean_df, method = "nearest", estimand = "ATT")
    )

  #Match
  matched_data <- match.data(ps_model)
  
  # Calculate ATT
  att <- with(matched_data, mean(student_ppnscal[college == 1]) - mean(student_ppnscal[college == 0]))
  
  # Balance summary
  bal <- bal.tab(ps_model, un = TRUE)$Balance
  unadj_smd <- abs(bal$Diff.Un)
  adj_smd <- abs(bal$Diff.Adj)
  
  # Calculate percent improvement
  percent_improvement <- ifelse(unadj_smd == 0, 0,
                                100 * (unadj_smd - adj_smd) / unadj_smd)
  
  # Mean percent improvement across covariates
  mean_percent_improvement <- mean(percent_improvement, na.rm = TRUE)
  
  # Proportion of covariates balanced after matching (SMD â‰¤ 0.1)
  prop_balanced <- mean(adj_smd <= 0.1, na.rm = TRUE)
  
  # Return results
  return(data.frame(
    att = att,
    prop_balanced = prop_balanced,
    mean_percent_improvement = mean_percent_improvement,
    covariates = I(list(selected_covariates)),
    percent_improvement = I(list(percent_improvement))
  ))
}

#Run 10000 times
sim_results2 <- map_dfr(1:nsim_models, ~ run_sim())


# Plot ATT v. proportion
ggplot(sim_results2, aes(x = prop_balanced, y = att)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", color = "steelblue") +
  labs(
    title = "ATT vs Proportion of Covariates Balanced",
    x = "Proportion Balanced (SMD â‰¤ 0.1)",
    y = "Estimated ATT"
  ) +
  theme_minimal()


# 10 random covariate balance plots (hint try gridExtra)
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!

set.seed(1234)
random_indices <- sample(1:nrow(sim_results2), 10)
random_models <- sim_results2[random_indices, ]
covariate_balance_plots <- lapply(1:nrow(random_models), function(i) {
  # Extract the model
  selected_covariates <- random_models$covariates[[i]]
  model <- random_models[i, ]
  
  # Create the formula
  formula <- as.formula(paste(treatment_var, "~", paste(selected_covariates, collapse = "+")))
  
  # Fit the propensity score model
  ps_model <- matchit(formula, data = clean_df, method = "nearest", estimand = "ATT")
  
  #balance table
  bal <- bal.tab(ps_model, un = TRUE)
  smds <- bal$Balance[, c("Diff.Un", "Diff.Adj")]
  smds$Covariate <- rownames(smds)
  smds$Model <- paste0("Model_", i)
  

  # Plot the balance
  plot <- love.plot(ps_model, 
            threshold = 0.1,
            abs = TRUE,
            var.order = "unadjusted",
            stat = "mean.diffs",
            title = paste("Covariate Balance for Model", i))
  
    return(list(smds = smds, plot = plot))
})

balance_df <- do.call(rbind, lapply(covariate_balance_plots, function(x) x$smds))

#install.packages("ggpubr")

library(gridExtra)
for (i in 1:10) {
  grid.arrange(grobs = list(covariate_balance_plots[[i]]$plot),
               top = paste("Covariate Balance Plot - Model", i))
}


#Number of simulations that result in any covariant balance below 0.1 threshold
num_simulations_below_threshold <- sum(sim_results2$prop_balanced > 0.1)


#summarize covariate balance across 10 random models

# Calculate the range for each covariate

balance_summary_stats <- balance_df %>%
  group_by(Covariate) %>%
  summarise(
    min_smd = min(Diff.Adj, na.rm = TRUE),
    max_smd = max(Diff.Adj, na.rm = TRUE)
  ) %>%
  mutate(smd_range = max_smd - min_smd)

# Now calculate the average of the ranges
average_smd_range <- mean(balance_summary_stats$smd_range, na.rm = TRUE)
print(average_smd_range)

sd_smd_range <- sd(balance_summary_stats$smd_range, na.rm = TRUE)
print(sd_smd_range)

```

## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}
    Your Answer: 8625 of my 10000 simulations result in a higher proportion of balanced covariates. It's important to note that the baseline model has no covariates below this threshold. While this is significant improvement over the baseline model, it is still concerning that about 15% of the models are not demonstrating balance on any of the covariates. This suggests that there is a lot of variability in the models and that some models are very poorly specified, such that the specification of the pscore model would have important implications for further analysis. 
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}
    Your Answer: I am somewhat concerned about the distribution of the ATTs, though it is hard to fully comment on this without interpreting the effect sizes at play. Here we see the ATTs are generally ranging between 1.5 and 1.7 on average. It does depend somewhat on the balance of the covariates, but the relationship here does not have a very clear pattern, as much of the mass is between 0.1 and 0.3 proportion of balanced covariates, and within this mass the ATT does not vary very much. This makes me a bit less concerned about the sensitivity of the estimates to the choice of model. 
    \item \textbf{Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}
    Your Answer: On average, the 10 randomly chosen models have similar balance for a given covariate. The mean distance between max SMD and min SMD for covariates across the 10 models is 0.07, which is a small range, but the standard deviation on the smd range is 0.17, indicating that many covariates have a larger range of balance across covariates, and that it is likely that these would not create covariates that are below the 0.1 threshold across different models. There are therefore concerns that the choice of model is important. 
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:

```{r}
# Remove post-treatment covariates

# Randomly select features

# Simulate random selection of features 10k+ times

# Fit  models and save ATTs, proportion of balanced covariates, and mean percent balance improvement

# Plot ATT v. proportion

# 10 random covariate balance plots (hint try gridExtra)
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!

run_sim_optimal <- function() {
# Simulate random selection of features 10k+ times
  # Randomly select a number of covariates
  num_covariates <- sample(1:length(covariates), 1)
  
  # Randomly select covariates
  selected_covariates <- sample(covariates, num_covariates)
  
  # Create formula for matching
  formula <- as.formula(paste(treatment_var, "~", paste(selected_covariates, collapse = "+")))

# Fit p-score models and save ATTs, proportion of balanced covariates, and mean percent balance improvement  
  # Fit the propensity score model
  optimal_model <- suppressWarnings(
    matchit(formula, data = clean_df, method = "optimal", estimand = "ATT")
    )

  #Match
  matched_data_optimal <- match.data(optimal_model)
  
  # Calculate ATT
  att <- with(matched_data_optimal, mean(student_ppnscal[college == 1]) - mean(student_ppnscal[college == 0]))
  
  # Balance summary
  bal <- bal.tab(optimal_model, un = TRUE)$Balance
  unadj_smd <- abs(bal$Diff.Un)
  adj_smd <- abs(bal$Diff.Adj)
  
  # Calculate percent improvement
  percent_improvement <- ifelse(unadj_smd == 0, 0,
                                100 * (unadj_smd - adj_smd) / unadj_smd)
  
  # Mean percent improvement across covariates
  mean_percent_improvement <- mean(percent_improvement, na.rm = TRUE)
  
  # Proportion of covariates balanced after matching (SMD â‰¤ 0.1)
  prop_balanced <- mean(adj_smd <= 0.1, na.rm = TRUE)
  
  # Return results
  return(data.frame(
    att = att,
    prop_balanced = prop_balanced,
    mean_percent_improvement = mean_percent_improvement,
    covariates = I(list(selected_covariates)),
    percent_improvement = I(list(percent_improvement))
  ))
}
#Run 10000 times
results_optimal <- map_dfr(1:nsim_models, ~ run_sim_optimal())


# Plot ATT v. proportion
ggplot(results_optimal, aes(x = prop_balanced, y = att)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess", color = "steelblue") +
  labs(
    title = "ATT vs Proportion of Covariates Balanced (Optimal Matching)",
    x = "Proportion Balanced (SMD â‰¤ 0.1)",
    y = "Estimated ATT"
  ) +
  theme_minimal()


# 10 random covariate balance plots (hint try gridExtra)
# Note: ggplot objects are finnicky so ask for help if you're struggling to automatically create them; consider using functions!

set.seed(1234)
random_indices <- sample(1:nrow(results_optimal), 10)
random_models <- results_optimal[random_indices, ]
covariate_balance_plots <- lapply(1:nrow(random_models), function(i) {
  # Extract the model
  selected_covariates <- random_models$covariates[[i]]
  model <- random_models[i, ]
  
  # Create the formula
  formula <- as.formula(paste(treatment_var, "~", paste(selected_covariates, collapse = "+")))
  
  # Fit the propensity score model
  ps_model <- matchit(formula, data = clean_df, method = "optimal", estimand = "ATT")
  
  # Plot the balance
  love.plot(ps_model, 
            threshold = 0.1,
            abs = TRUE,
            var.order = "unadjusted",
            stat = "mean.diffs",
            title = paste("Covariate Balance for (Optimal) Model", i))
})




```

```{r}
#Number of simulations that result in any covariant balance below 0.1 threshold
num_simulations_below_threshold_optimal <- sum(results_optimal$prop_balanced > 0.1)
num_simulations_below_threshold_optimal


# Visualization for distributions of percent improvement

# Add method labels
psm_results <- sim_results2 %>%
  mutate(Method = "Propensity Score Matching")

new_method_results <- results_optimal %>%
  mutate(Method = "Optimal Matching")

# Combine into one dataframe
combined_results <- bind_rows(psm_results, new_method_results)
combined_results <- combined_results %>%
  filter(!is.na(mean_percent_improvement))

# Plot distributions
ggplot(combined_results, aes(x = mean_percent_improvement, fill = Method)) +
  geom_density(alpha = 0.5) +
  labs(
    title = "Distribution of Percent Improvement in Covariate Balance",
    x = "Mean Percent Improvement in SMD",
    y = "Density",
    fill = "Matching Method"
  ) +
  theme_minimal()

```

## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}
     Your Answer: Yes the alternative method has much better balance across the covariates, with runs generally obtaining 50 to 100% of their covariates below the threshold of 0.1 SMD. 
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}
    Your Answer: The optimal matching method did a lot better. 
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item \textbf{Why might it be a good idea to do matching even if we have a randomized or as-if-random design?}
    Your Answer: Matching can address the imbalance that may occur due to change in randomized studies and the imbalance that is more likely to occur in observational studies. Matching is helpful because it also reduces variance in the estimates of treatment effects, which can be particularly important when the sample size is small as is more likely in randomized trials. 
    \item \textbf{The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?}
    Your Answer: There may be some advantages to using other machine learning algorithms to estimate propensity scores. For example, ML algorithms are better at capturing complex interactions and non-linear relationships between covariates and treatment assignment and can also more systematically consider many potential features and interactions in order to determine the model that best fits the data and reducing dimensionality. 
\end{enumerate}